{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa0b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('max.columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45134843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>Col11</th>\n",
       "      <th>Col12</th>\n",
       "      <th>Col13</th>\n",
       "      <th>Col14</th>\n",
       "      <th>Col15</th>\n",
       "      <th>Col16</th>\n",
       "      <th>Col17</th>\n",
       "      <th>Col18</th>\n",
       "      <th>Col19</th>\n",
       "      <th>Col20</th>\n",
       "      <th>Col21</th>\n",
       "      <th>Col22</th>\n",
       "      <th>Col23</th>\n",
       "      <th>Col24</th>\n",
       "      <th>Col25</th>\n",
       "      <th>Col26</th>\n",
       "      <th>Col27</th>\n",
       "      <th>Col28</th>\n",
       "      <th>Col29</th>\n",
       "      <th>Col30</th>\n",
       "      <th>Col31</th>\n",
       "      <th>Col32</th>\n",
       "      <th>Col33</th>\n",
       "      <th>Col34</th>\n",
       "      <th>Col35</th>\n",
       "      <th>Col36</th>\n",
       "      <th>Col37</th>\n",
       "      <th>Col38</th>\n",
       "      <th>Col39</th>\n",
       "      <th>Col40</th>\n",
       "      <th>Col41</th>\n",
       "      <th>Col42</th>\n",
       "      <th>Col43</th>\n",
       "      <th>Col44</th>\n",
       "      <th>Col45</th>\n",
       "      <th>Col46</th>\n",
       "      <th>Col47</th>\n",
       "      <th>Col48</th>\n",
       "      <th>Col49</th>\n",
       "      <th>Col50</th>\n",
       "      <th>Col51</th>\n",
       "      <th>Col52</th>\n",
       "      <th>Col53</th>\n",
       "      <th>Col54</th>\n",
       "      <th>Col55</th>\n",
       "      <th>Col56</th>\n",
       "      <th>Col57</th>\n",
       "      <th>Col58</th>\n",
       "      <th>Col59</th>\n",
       "      <th>Col60</th>\n",
       "      <th>Col61</th>\n",
       "      <th>Col62</th>\n",
       "      <th>Col63</th>\n",
       "      <th>Col64</th>\n",
       "      <th>Col65</th>\n",
       "      <th>Col66</th>\n",
       "      <th>Col67</th>\n",
       "      <th>Col68</th>\n",
       "      <th>Col69</th>\n",
       "      <th>Col70</th>\n",
       "      <th>Col71</th>\n",
       "      <th>Col72</th>\n",
       "      <th>Col73</th>\n",
       "      <th>Col74</th>\n",
       "      <th>Col75</th>\n",
       "      <th>Col76</th>\n",
       "      <th>Col77</th>\n",
       "      <th>Col78</th>\n",
       "      <th>Col79</th>\n",
       "      <th>Col80</th>\n",
       "      <th>Col81</th>\n",
       "      <th>Col82</th>\n",
       "      <th>Col83</th>\n",
       "      <th>Col84</th>\n",
       "      <th>Col85</th>\n",
       "      <th>Col86</th>\n",
       "      <th>Col87</th>\n",
       "      <th>Col88</th>\n",
       "      <th>Col89</th>\n",
       "      <th>Col90</th>\n",
       "      <th>Col91</th>\n",
       "      <th>Col92</th>\n",
       "      <th>Col93</th>\n",
       "      <th>Col94</th>\n",
       "      <th>Col95</th>\n",
       "      <th>Col96</th>\n",
       "      <th>Col97</th>\n",
       "      <th>Col98</th>\n",
       "      <th>Col99</th>\n",
       "      <th>Col100</th>\n",
       "      <th>Col101</th>\n",
       "      <th>Col102</th>\n",
       "      <th>Col103</th>\n",
       "      <th>Col104</th>\n",
       "      <th>Col105</th>\n",
       "      <th>Col106</th>\n",
       "      <th>Col107</th>\n",
       "      <th>Col108</th>\n",
       "      <th>Col109</th>\n",
       "      <th>Col110</th>\n",
       "      <th>Col111</th>\n",
       "      <th>Col112</th>\n",
       "      <th>Col113</th>\n",
       "      <th>Col114</th>\n",
       "      <th>Col115</th>\n",
       "      <th>Col116</th>\n",
       "      <th>Col117</th>\n",
       "      <th>Col118</th>\n",
       "      <th>Col119</th>\n",
       "      <th>Col120</th>\n",
       "      <th>Col121</th>\n",
       "      <th>Col122</th>\n",
       "      <th>Col123</th>\n",
       "      <th>Col124</th>\n",
       "      <th>Col125</th>\n",
       "      <th>Col126</th>\n",
       "      <th>Col127</th>\n",
       "      <th>Col128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.083383</td>\n",
       "      <td>0.103556</td>\n",
       "      <td>0.122871</td>\n",
       "      <td>0.150950</td>\n",
       "      <td>0.175772</td>\n",
       "      <td>0.201790</td>\n",
       "      <td>0.224542</td>\n",
       "      <td>0.260662</td>\n",
       "      <td>0.295880</td>\n",
       "      <td>0.324528</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.391972</td>\n",
       "      <td>0.432689</td>\n",
       "      <td>0.464556</td>\n",
       "      <td>0.500096</td>\n",
       "      <td>0.532524</td>\n",
       "      <td>0.570259</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>0.630355</td>\n",
       "      <td>0.655732</td>\n",
       "      <td>0.679630</td>\n",
       "      <td>0.979785</td>\n",
       "      <td>0.784710</td>\n",
       "      <td>0.772336</td>\n",
       "      <td>0.746723</td>\n",
       "      <td>0.772066</td>\n",
       "      <td>0.772647</td>\n",
       "      <td>0.767681</td>\n",
       "      <td>0.775952</td>\n",
       "      <td>0.770374</td>\n",
       "      <td>0.760721</td>\n",
       "      <td>0.760321</td>\n",
       "      <td>0.765592</td>\n",
       "      <td>0.724601</td>\n",
       "      <td>0.703462</td>\n",
       "      <td>0.679174</td>\n",
       "      <td>0.650955</td>\n",
       "      <td>0.633548</td>\n",
       "      <td>0.597261</td>\n",
       "      <td>0.587476</td>\n",
       "      <td>0.541696</td>\n",
       "      <td>0.497535</td>\n",
       "      <td>0.466749</td>\n",
       "      <td>0.434485</td>\n",
       "      <td>0.392753</td>\n",
       "      <td>0.366640</td>\n",
       "      <td>0.332634</td>\n",
       "      <td>0.296339</td>\n",
       "      <td>0.263667</td>\n",
       "      <td>0.231731</td>\n",
       "      <td>0.201444</td>\n",
       "      <td>0.176189</td>\n",
       "      <td>0.151081</td>\n",
       "      <td>0.122737</td>\n",
       "      <td>0.101760</td>\n",
       "      <td>0.090976</td>\n",
       "      <td>0.213345</td>\n",
       "      <td>0.090705</td>\n",
       "      <td>0.075370</td>\n",
       "      <td>0.051831</td>\n",
       "      <td>0.051550</td>\n",
       "      <td>0.060180</td>\n",
       "      <td>0.062859</td>\n",
       "      <td>0.075351</td>\n",
       "      <td>0.083303</td>\n",
       "      <td>0.105279</td>\n",
       "      <td>0.122436</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.173012</td>\n",
       "      <td>0.202367</td>\n",
       "      <td>0.228276</td>\n",
       "      <td>0.261486</td>\n",
       "      <td>0.291620</td>\n",
       "      <td>0.332648</td>\n",
       "      <td>0.370159</td>\n",
       "      <td>0.394883</td>\n",
       "      <td>0.437931</td>\n",
       "      <td>0.472219</td>\n",
       "      <td>0.503045</td>\n",
       "      <td>0.644568</td>\n",
       "      <td>0.679630</td>\n",
       "      <td>0.719550</td>\n",
       "      <td>0.761869</td>\n",
       "      <td>0.791958</td>\n",
       "      <td>0.819152</td>\n",
       "      <td>0.852555</td>\n",
       "      <td>0.879120</td>\n",
       "      <td>0.893838</td>\n",
       "      <td>0.907680</td>\n",
       "      <td>0.925014</td>\n",
       "      <td>0.928071</td>\n",
       "      <td>0.940588</td>\n",
       "      <td>0.938938</td>\n",
       "      <td>0.929698</td>\n",
       "      <td>0.925659</td>\n",
       "      <td>0.911138</td>\n",
       "      <td>0.901213</td>\n",
       "      <td>0.875550</td>\n",
       "      <td>0.855242</td>\n",
       "      <td>0.825250</td>\n",
       "      <td>0.794845</td>\n",
       "      <td>0.761984</td>\n",
       "      <td>0.728347</td>\n",
       "      <td>0.688468</td>\n",
       "      <td>0.646426</td>\n",
       "      <td>0.609315</td>\n",
       "      <td>0.566886</td>\n",
       "      <td>0.527835</td>\n",
       "      <td>0.478071</td>\n",
       "      <td>0.439570</td>\n",
       "      <td>0.394203</td>\n",
       "      <td>0.358181</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.282667</td>\n",
       "      <td>0.239611</td>\n",
       "      <td>0.212284</td>\n",
       "      <td>0.184046</td>\n",
       "      <td>0.149749</td>\n",
       "      <td>0.124382</td>\n",
       "      <td>0.109429</td>\n",
       "      <td>0.095443</td>\n",
       "      <td>0.078435</td>\n",
       "      <td>0.074675</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.063165</td>\n",
       "      <td>0.263747</td>\n",
       "      <td>0.114574</td>\n",
       "      <td>0.400647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.353586</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>0.296272</td>\n",
       "      <td>0.275924</td>\n",
       "      <td>0.247453</td>\n",
       "      <td>0.220534</td>\n",
       "      <td>0.189593</td>\n",
       "      <td>0.173912</td>\n",
       "      <td>0.156294</td>\n",
       "      <td>0.135051</td>\n",
       "      <td>0.124699</td>\n",
       "      <td>0.110056</td>\n",
       "      <td>0.106561</td>\n",
       "      <td>0.097347</td>\n",
       "      <td>0.099621</td>\n",
       "      <td>0.097448</td>\n",
       "      <td>0.096008</td>\n",
       "      <td>0.104537</td>\n",
       "      <td>0.120322</td>\n",
       "      <td>0.134268</td>\n",
       "      <td>0.144966</td>\n",
       "      <td>0.166781</td>\n",
       "      <td>0.187069</td>\n",
       "      <td>0.211960</td>\n",
       "      <td>0.227449</td>\n",
       "      <td>0.261966</td>\n",
       "      <td>0.288186</td>\n",
       "      <td>0.314007</td>\n",
       "      <td>0.343539</td>\n",
       "      <td>0.370656</td>\n",
       "      <td>0.403981</td>\n",
       "      <td>0.443430</td>\n",
       "      <td>0.483101</td>\n",
       "      <td>0.498855</td>\n",
       "      <td>0.528519</td>\n",
       "      <td>0.555722</td>\n",
       "      <td>0.578098</td>\n",
       "      <td>0.612729</td>\n",
       "      <td>0.629794</td>\n",
       "      <td>0.674864</td>\n",
       "      <td>0.680873</td>\n",
       "      <td>0.682558</td>\n",
       "      <td>0.699986</td>\n",
       "      <td>0.713939</td>\n",
       "      <td>0.716193</td>\n",
       "      <td>0.734215</td>\n",
       "      <td>0.743951</td>\n",
       "      <td>0.730587</td>\n",
       "      <td>0.725147</td>\n",
       "      <td>0.714117</td>\n",
       "      <td>0.714720</td>\n",
       "      <td>0.698707</td>\n",
       "      <td>0.679689</td>\n",
       "      <td>0.658591</td>\n",
       "      <td>0.635939</td>\n",
       "      <td>0.620088</td>\n",
       "      <td>0.614209</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.542166</td>\n",
       "      <td>0.512280</td>\n",
       "      <td>0.477366</td>\n",
       "      <td>0.449985</td>\n",
       "      <td>0.416656</td>\n",
       "      <td>0.387991</td>\n",
       "      <td>0.352324</td>\n",
       "      <td>0.329852</td>\n",
       "      <td>0.296575</td>\n",
       "      <td>0.269225</td>\n",
       "      <td>0.244826</td>\n",
       "      <td>0.221323</td>\n",
       "      <td>0.193631</td>\n",
       "      <td>0.174319</td>\n",
       "      <td>0.153871</td>\n",
       "      <td>0.138538</td>\n",
       "      <td>0.126047</td>\n",
       "      <td>0.111024</td>\n",
       "      <td>0.107718</td>\n",
       "      <td>0.098914</td>\n",
       "      <td>0.100119</td>\n",
       "      <td>0.117977</td>\n",
       "      <td>0.114455</td>\n",
       "      <td>0.126901</td>\n",
       "      <td>0.145431</td>\n",
       "      <td>0.162011</td>\n",
       "      <td>0.174786</td>\n",
       "      <td>0.200064</td>\n",
       "      <td>0.224692</td>\n",
       "      <td>0.253898</td>\n",
       "      <td>0.276524</td>\n",
       "      <td>0.313691</td>\n",
       "      <td>0.346178</td>\n",
       "      <td>0.384944</td>\n",
       "      <td>0.415450</td>\n",
       "      <td>0.446948</td>\n",
       "      <td>0.492227</td>\n",
       "      <td>0.531260</td>\n",
       "      <td>0.568777</td>\n",
       "      <td>0.603168</td>\n",
       "      <td>0.642756</td>\n",
       "      <td>0.674139</td>\n",
       "      <td>0.705839</td>\n",
       "      <td>0.737792</td>\n",
       "      <td>0.768454</td>\n",
       "      <td>0.790191</td>\n",
       "      <td>0.812307</td>\n",
       "      <td>0.836668</td>\n",
       "      <td>0.851576</td>\n",
       "      <td>0.866683</td>\n",
       "      <td>0.870724</td>\n",
       "      <td>0.882243</td>\n",
       "      <td>0.878812</td>\n",
       "      <td>0.882164</td>\n",
       "      <td>0.885688</td>\n",
       "      <td>0.873178</td>\n",
       "      <td>0.854639</td>\n",
       "      <td>0.838139</td>\n",
       "      <td>0.825352</td>\n",
       "      <td>0.800076</td>\n",
       "      <td>0.775432</td>\n",
       "      <td>0.746250</td>\n",
       "      <td>0.723682</td>\n",
       "      <td>0.686454</td>\n",
       "      <td>0.653969</td>\n",
       "      <td>0.615141</td>\n",
       "      <td>0.584738</td>\n",
       "      <td>0.553114</td>\n",
       "      <td>0.507915</td>\n",
       "      <td>0.468949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095419</td>\n",
       "      <td>0.115687</td>\n",
       "      <td>0.134050</td>\n",
       "      <td>0.160488</td>\n",
       "      <td>0.185254</td>\n",
       "      <td>0.211083</td>\n",
       "      <td>0.232131</td>\n",
       "      <td>0.270633</td>\n",
       "      <td>0.302941</td>\n",
       "      <td>0.329109</td>\n",
       "      <td>0.370945</td>\n",
       "      <td>0.393027</td>\n",
       "      <td>0.433808</td>\n",
       "      <td>0.462506</td>\n",
       "      <td>0.495952</td>\n",
       "      <td>0.529087</td>\n",
       "      <td>0.564644</td>\n",
       "      <td>0.586140</td>\n",
       "      <td>0.624351</td>\n",
       "      <td>0.645001</td>\n",
       "      <td>0.669031</td>\n",
       "      <td>0.697760</td>\n",
       "      <td>0.717937</td>\n",
       "      <td>0.733818</td>\n",
       "      <td>0.733001</td>\n",
       "      <td>0.756132</td>\n",
       "      <td>0.752672</td>\n",
       "      <td>0.746319</td>\n",
       "      <td>0.756885</td>\n",
       "      <td>0.753025</td>\n",
       "      <td>0.745845</td>\n",
       "      <td>0.745482</td>\n",
       "      <td>0.748884</td>\n",
       "      <td>0.708096</td>\n",
       "      <td>0.686242</td>\n",
       "      <td>0.666051</td>\n",
       "      <td>0.637045</td>\n",
       "      <td>0.620421</td>\n",
       "      <td>0.586692</td>\n",
       "      <td>0.574394</td>\n",
       "      <td>0.534374</td>\n",
       "      <td>0.490148</td>\n",
       "      <td>0.459870</td>\n",
       "      <td>0.431276</td>\n",
       "      <td>0.390070</td>\n",
       "      <td>0.367492</td>\n",
       "      <td>0.336692</td>\n",
       "      <td>0.298630</td>\n",
       "      <td>0.270139</td>\n",
       "      <td>0.238408</td>\n",
       "      <td>0.207696</td>\n",
       "      <td>0.187901</td>\n",
       "      <td>0.161584</td>\n",
       "      <td>0.134591</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.101217</td>\n",
       "      <td>0.096110</td>\n",
       "      <td>0.077745</td>\n",
       "      <td>0.080501</td>\n",
       "      <td>0.068431</td>\n",
       "      <td>0.069035</td>\n",
       "      <td>0.072498</td>\n",
       "      <td>0.074282</td>\n",
       "      <td>0.086026</td>\n",
       "      <td>0.094625</td>\n",
       "      <td>0.116709</td>\n",
       "      <td>0.134746</td>\n",
       "      <td>0.157458</td>\n",
       "      <td>0.184183</td>\n",
       "      <td>0.211012</td>\n",
       "      <td>0.236778</td>\n",
       "      <td>0.269691</td>\n",
       "      <td>0.297031</td>\n",
       "      <td>0.338024</td>\n",
       "      <td>0.374058</td>\n",
       "      <td>0.396542</td>\n",
       "      <td>0.424092</td>\n",
       "      <td>0.419071</td>\n",
       "      <td>0.418398</td>\n",
       "      <td>0.502887</td>\n",
       "      <td>0.498856</td>\n",
       "      <td>0.502334</td>\n",
       "      <td>0.510534</td>\n",
       "      <td>0.509377</td>\n",
       "      <td>0.506423</td>\n",
       "      <td>0.512530</td>\n",
       "      <td>0.513782</td>\n",
       "      <td>0.513323</td>\n",
       "      <td>0.508779</td>\n",
       "      <td>0.513844</td>\n",
       "      <td>0.511625</td>\n",
       "      <td>0.516758</td>\n",
       "      <td>0.511680</td>\n",
       "      <td>0.505718</td>\n",
       "      <td>0.512800</td>\n",
       "      <td>0.513804</td>\n",
       "      <td>0.512731</td>\n",
       "      <td>0.510337</td>\n",
       "      <td>0.513910</td>\n",
       "      <td>0.510933</td>\n",
       "      <td>0.507928</td>\n",
       "      <td>0.509681</td>\n",
       "      <td>0.507167</td>\n",
       "      <td>0.504482</td>\n",
       "      <td>0.501387</td>\n",
       "      <td>0.504091</td>\n",
       "      <td>0.502154</td>\n",
       "      <td>0.503805</td>\n",
       "      <td>0.495279</td>\n",
       "      <td>0.499354</td>\n",
       "      <td>0.493262</td>\n",
       "      <td>0.497225</td>\n",
       "      <td>0.501470</td>\n",
       "      <td>0.497814</td>\n",
       "      <td>0.489541</td>\n",
       "      <td>0.490670</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>0.487575</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.486791</td>\n",
       "      <td>0.491376</td>\n",
       "      <td>0.486201</td>\n",
       "      <td>0.488421</td>\n",
       "      <td>0.483289</td>\n",
       "      <td>0.488394</td>\n",
       "      <td>0.494236</td>\n",
       "      <td>0.487169</td>\n",
       "      <td>0.486169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.698665</td>\n",
       "      <td>0.718425</td>\n",
       "      <td>0.740934</td>\n",
       "      <td>0.772865</td>\n",
       "      <td>0.781895</td>\n",
       "      <td>0.780248</td>\n",
       "      <td>0.762148</td>\n",
       "      <td>0.758475</td>\n",
       "      <td>0.748144</td>\n",
       "      <td>0.720218</td>\n",
       "      <td>0.709707</td>\n",
       "      <td>0.673525</td>\n",
       "      <td>0.660023</td>\n",
       "      <td>0.639333</td>\n",
       "      <td>0.621777</td>\n",
       "      <td>0.602368</td>\n",
       "      <td>0.586929</td>\n",
       "      <td>0.552057</td>\n",
       "      <td>0.530111</td>\n",
       "      <td>0.493121</td>\n",
       "      <td>0.451353</td>\n",
       "      <td>0.416271</td>\n",
       "      <td>0.374120</td>\n",
       "      <td>0.335726</td>\n",
       "      <td>0.291067</td>\n",
       "      <td>0.269485</td>\n",
       "      <td>0.242768</td>\n",
       "      <td>0.219690</td>\n",
       "      <td>0.198198</td>\n",
       "      <td>0.176056</td>\n",
       "      <td>0.163021</td>\n",
       "      <td>0.149787</td>\n",
       "      <td>0.125578</td>\n",
       "      <td>0.102551</td>\n",
       "      <td>0.087596</td>\n",
       "      <td>0.071765</td>\n",
       "      <td>0.057493</td>\n",
       "      <td>0.059611</td>\n",
       "      <td>0.056189</td>\n",
       "      <td>0.072203</td>\n",
       "      <td>0.084392</td>\n",
       "      <td>0.104498</td>\n",
       "      <td>0.126836</td>\n",
       "      <td>0.150832</td>\n",
       "      <td>0.162993</td>\n",
       "      <td>0.187583</td>\n",
       "      <td>0.205303</td>\n",
       "      <td>0.224360</td>\n",
       "      <td>0.246030</td>\n",
       "      <td>0.269687</td>\n",
       "      <td>0.300668</td>\n",
       "      <td>0.337895</td>\n",
       "      <td>0.375798</td>\n",
       "      <td>0.412580</td>\n",
       "      <td>0.451884</td>\n",
       "      <td>0.497154</td>\n",
       "      <td>0.548821</td>\n",
       "      <td>0.563481</td>\n",
       "      <td>0.587495</td>\n",
       "      <td>0.608713</td>\n",
       "      <td>0.621066</td>\n",
       "      <td>0.641033</td>\n",
       "      <td>0.657623</td>\n",
       "      <td>0.679261</td>\n",
       "      <td>0.696407</td>\n",
       "      <td>0.728218</td>\n",
       "      <td>0.741674</td>\n",
       "      <td>0.754156</td>\n",
       "      <td>0.773425</td>\n",
       "      <td>0.783040</td>\n",
       "      <td>0.778321</td>\n",
       "      <td>0.760194</td>\n",
       "      <td>0.736431</td>\n",
       "      <td>0.738694</td>\n",
       "      <td>0.717196</td>\n",
       "      <td>0.679276</td>\n",
       "      <td>0.667349</td>\n",
       "      <td>0.649871</td>\n",
       "      <td>0.624775</td>\n",
       "      <td>0.729130</td>\n",
       "      <td>0.699493</td>\n",
       "      <td>0.670210</td>\n",
       "      <td>0.640456</td>\n",
       "      <td>0.594668</td>\n",
       "      <td>0.543652</td>\n",
       "      <td>0.498765</td>\n",
       "      <td>0.449014</td>\n",
       "      <td>0.401807</td>\n",
       "      <td>0.353460</td>\n",
       "      <td>0.322486</td>\n",
       "      <td>0.291414</td>\n",
       "      <td>0.269180</td>\n",
       "      <td>0.239442</td>\n",
       "      <td>0.212153</td>\n",
       "      <td>0.198469</td>\n",
       "      <td>0.179317</td>\n",
       "      <td>0.147562</td>\n",
       "      <td>0.123794</td>\n",
       "      <td>0.106391</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.070197</td>\n",
       "      <td>0.071778</td>\n",
       "      <td>0.068560</td>\n",
       "      <td>0.084542</td>\n",
       "      <td>0.100753</td>\n",
       "      <td>0.128092</td>\n",
       "      <td>0.154373</td>\n",
       "      <td>0.183158</td>\n",
       "      <td>0.198231</td>\n",
       "      <td>0.225472</td>\n",
       "      <td>0.242533</td>\n",
       "      <td>0.270998</td>\n",
       "      <td>0.300569</td>\n",
       "      <td>0.329826</td>\n",
       "      <td>0.359554</td>\n",
       "      <td>0.405393</td>\n",
       "      <td>0.456404</td>\n",
       "      <td>0.501284</td>\n",
       "      <td>0.551075</td>\n",
       "      <td>0.598253</td>\n",
       "      <td>0.646652</td>\n",
       "      <td>0.677641</td>\n",
       "      <td>0.708715</td>\n",
       "      <td>0.730951</td>\n",
       "      <td>0.760745</td>\n",
       "      <td>0.788017</td>\n",
       "      <td>0.801661</td>\n",
       "      <td>0.820882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.462267</td>\n",
       "      <td>0.496878</td>\n",
       "      <td>0.534562</td>\n",
       "      <td>0.579235</td>\n",
       "      <td>0.610549</td>\n",
       "      <td>0.637590</td>\n",
       "      <td>0.649317</td>\n",
       "      <td>0.682783</td>\n",
       "      <td>0.708635</td>\n",
       "      <td>0.721439</td>\n",
       "      <td>0.751087</td>\n",
       "      <td>0.746780</td>\n",
       "      <td>0.761408</td>\n",
       "      <td>0.762728</td>\n",
       "      <td>0.763598</td>\n",
       "      <td>0.765547</td>\n",
       "      <td>0.774494</td>\n",
       "      <td>0.756916</td>\n",
       "      <td>0.755456</td>\n",
       "      <td>0.737419</td>\n",
       "      <td>0.719213</td>\n",
       "      <td>0.707400</td>\n",
       "      <td>0.687186</td>\n",
       "      <td>0.659505</td>\n",
       "      <td>0.620066</td>\n",
       "      <td>0.604594</td>\n",
       "      <td>0.564978</td>\n",
       "      <td>0.530373</td>\n",
       "      <td>0.498909</td>\n",
       "      <td>0.460349</td>\n",
       "      <td>0.428045</td>\n",
       "      <td>0.400062</td>\n",
       "      <td>0.372236</td>\n",
       "      <td>0.327639</td>\n",
       "      <td>0.293100</td>\n",
       "      <td>0.262765</td>\n",
       "      <td>0.227167</td>\n",
       "      <td>0.202954</td>\n",
       "      <td>0.172204</td>\n",
       "      <td>0.152319</td>\n",
       "      <td>0.124275</td>\n",
       "      <td>0.106362</td>\n",
       "      <td>0.086787</td>\n",
       "      <td>0.081120</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.063997</td>\n",
       "      <td>0.056170</td>\n",
       "      <td>0.060531</td>\n",
       "      <td>0.060321</td>\n",
       "      <td>0.065313</td>\n",
       "      <td>0.070086</td>\n",
       "      <td>0.089750</td>\n",
       "      <td>0.106767</td>\n",
       "      <td>0.122198</td>\n",
       "      <td>0.141988</td>\n",
       "      <td>0.173844</td>\n",
       "      <td>0.206481</td>\n",
       "      <td>0.228812</td>\n",
       "      <td>0.264099</td>\n",
       "      <td>0.291525</td>\n",
       "      <td>0.323659</td>\n",
       "      <td>0.362346</td>\n",
       "      <td>0.392780</td>\n",
       "      <td>0.416926</td>\n",
       "      <td>0.408196</td>\n",
       "      <td>0.419126</td>\n",
       "      <td>0.415662</td>\n",
       "      <td>0.416564</td>\n",
       "      <td>0.422136</td>\n",
       "      <td>0.426737</td>\n",
       "      <td>0.424942</td>\n",
       "      <td>0.424252</td>\n",
       "      <td>0.420807</td>\n",
       "      <td>0.432660</td>\n",
       "      <td>0.434141</td>\n",
       "      <td>0.423763</td>\n",
       "      <td>0.430360</td>\n",
       "      <td>0.429589</td>\n",
       "      <td>0.426454</td>\n",
       "      <td>0.511867</td>\n",
       "      <td>0.506561</td>\n",
       "      <td>0.508878</td>\n",
       "      <td>0.515521</td>\n",
       "      <td>0.512974</td>\n",
       "      <td>0.508473</td>\n",
       "      <td>0.512937</td>\n",
       "      <td>0.512677</td>\n",
       "      <td>0.510651</td>\n",
       "      <td>0.504567</td>\n",
       "      <td>0.508472</td>\n",
       "      <td>0.504779</td>\n",
       "      <td>0.508609</td>\n",
       "      <td>0.502063</td>\n",
       "      <td>0.494843</td>\n",
       "      <td>0.500868</td>\n",
       "      <td>0.501093</td>\n",
       "      <td>0.499040</td>\n",
       "      <td>0.496161</td>\n",
       "      <td>0.499173</td>\n",
       "      <td>0.495863</td>\n",
       "      <td>0.492577</td>\n",
       "      <td>0.494096</td>\n",
       "      <td>0.491377</td>\n",
       "      <td>0.489089</td>\n",
       "      <td>0.486187</td>\n",
       "      <td>0.489559</td>\n",
       "      <td>0.488141</td>\n",
       "      <td>0.490440</td>\n",
       "      <td>0.482910</td>\n",
       "      <td>0.487890</td>\n",
       "      <td>0.482879</td>\n",
       "      <td>0.488122</td>\n",
       "      <td>0.493356</td>\n",
       "      <td>0.491004</td>\n",
       "      <td>0.484357</td>\n",
       "      <td>0.486947</td>\n",
       "      <td>0.491515</td>\n",
       "      <td>0.487117</td>\n",
       "      <td>0.487343</td>\n",
       "      <td>0.489267</td>\n",
       "      <td>0.495464</td>\n",
       "      <td>0.491593</td>\n",
       "      <td>0.495359</td>\n",
       "      <td>0.491531</td>\n",
       "      <td>0.498033</td>\n",
       "      <td>0.505202</td>\n",
       "      <td>0.500369</td>\n",
       "      <td>0.516898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Col1      Col2      Col3      Col4      Col5      Col6      Col7  \\\n",
       "0  0.083383  0.103556  0.122871  0.150950  0.175772  0.201790  0.224542   \n",
       "1  0.353586  0.325441  0.296272  0.275924  0.247453  0.220534  0.189593   \n",
       "2  0.095419  0.115687  0.134050  0.160488  0.185254  0.211083  0.232131   \n",
       "3  0.698665  0.718425  0.740934  0.772865  0.781895  0.780248  0.762148   \n",
       "4  0.462267  0.496878  0.534562  0.579235  0.610549  0.637590  0.649317   \n",
       "\n",
       "       Col8      Col9     Col10     Col11     Col12     Col13     Col14  \\\n",
       "0  0.260662  0.295880  0.324528  0.365533  0.391972  0.432689  0.464556   \n",
       "1  0.173912  0.156294  0.135051  0.124699  0.110056  0.106561  0.097347   \n",
       "2  0.270633  0.302941  0.329109  0.370945  0.393027  0.433808  0.462506   \n",
       "3  0.758475  0.748144  0.720218  0.709707  0.673525  0.660023  0.639333   \n",
       "4  0.682783  0.708635  0.721439  0.751087  0.746780  0.761408  0.762728   \n",
       "\n",
       "      Col15     Col16     Col17     Col18     Col19     Col20     Col21  \\\n",
       "0  0.500096  0.532524  0.570259  0.592834  0.630355  0.655732  0.679630   \n",
       "1  0.099621  0.097448  0.096008  0.104537  0.120322  0.134268  0.144966   \n",
       "2  0.495952  0.529087  0.564644  0.586140  0.624351  0.645001  0.669031   \n",
       "3  0.621777  0.602368  0.586929  0.552057  0.530111  0.493121  0.451353   \n",
       "4  0.763598  0.765547  0.774494  0.756916  0.755456  0.737419  0.719213   \n",
       "\n",
       "      Col22     Col23     Col24     Col25     Col26     Col27     Col28  \\\n",
       "0  0.979785  0.784710  0.772336  0.746723  0.772066  0.772647  0.767681   \n",
       "1  0.166781  0.187069  0.211960  0.227449  0.261966  0.288186  0.314007   \n",
       "2  0.697760  0.717937  0.733818  0.733001  0.756132  0.752672  0.746319   \n",
       "3  0.416271  0.374120  0.335726  0.291067  0.269485  0.242768  0.219690   \n",
       "4  0.707400  0.687186  0.659505  0.620066  0.604594  0.564978  0.530373   \n",
       "\n",
       "      Col29     Col30     Col31     Col32     Col33     Col34     Col35  \\\n",
       "0  0.775952  0.770374  0.760721  0.760321  0.765592  0.724601  0.703462   \n",
       "1  0.343539  0.370656  0.403981  0.443430  0.483101  0.498855  0.528519   \n",
       "2  0.756885  0.753025  0.745845  0.745482  0.748884  0.708096  0.686242   \n",
       "3  0.198198  0.176056  0.163021  0.149787  0.125578  0.102551  0.087596   \n",
       "4  0.498909  0.460349  0.428045  0.400062  0.372236  0.327639  0.293100   \n",
       "\n",
       "      Col36     Col37     Col38     Col39     Col40     Col41     Col42  \\\n",
       "0  0.679174  0.650955  0.633548  0.597261  0.587476  0.541696  0.497535   \n",
       "1  0.555722  0.578098  0.612729  0.629794  0.674864  0.680873  0.682558   \n",
       "2  0.666051  0.637045  0.620421  0.586692  0.574394  0.534374  0.490148   \n",
       "3  0.071765  0.057493  0.059611  0.056189  0.072203  0.084392  0.104498   \n",
       "4  0.262765  0.227167  0.202954  0.172204  0.152319  0.124275  0.106362   \n",
       "\n",
       "      Col43     Col44     Col45     Col46     Col47     Col48     Col49  \\\n",
       "0  0.466749  0.434485  0.392753  0.366640  0.332634  0.296339  0.263667   \n",
       "1  0.699986  0.713939  0.716193  0.734215  0.743951  0.730587  0.725147   \n",
       "2  0.459870  0.431276  0.390070  0.367492  0.336692  0.298630  0.270139   \n",
       "3  0.126836  0.150832  0.162993  0.187583  0.205303  0.224360  0.246030   \n",
       "4  0.086787  0.081120  0.060583  0.063997  0.056170  0.060531  0.060321   \n",
       "\n",
       "      Col50     Col51     Col52     Col53     Col54     Col55     Col56  \\\n",
       "0  0.231731  0.201444  0.176189  0.151081  0.122737  0.101760  0.090976   \n",
       "1  0.714117  0.714720  0.698707  0.679689  0.658591  0.635939  0.620088   \n",
       "2  0.238408  0.207696  0.187901  0.161584  0.134591  0.116075  0.101217   \n",
       "3  0.269687  0.300668  0.337895  0.375798  0.412580  0.451884  0.497154   \n",
       "4  0.065313  0.070086  0.089750  0.106767  0.122198  0.141988  0.173844   \n",
       "\n",
       "      Col57     Col58     Col59     Col60     Col61     Col62     Col63  \\\n",
       "0  0.213345  0.090705  0.075370  0.051831  0.051550  0.060180  0.062859   \n",
       "1  0.614209  0.570810  0.542166  0.512280  0.477366  0.449985  0.416656   \n",
       "2  0.096110  0.077745  0.080501  0.068431  0.069035  0.072498  0.074282   \n",
       "3  0.548821  0.563481  0.587495  0.608713  0.621066  0.641033  0.657623   \n",
       "4  0.206481  0.228812  0.264099  0.291525  0.323659  0.362346  0.392780   \n",
       "\n",
       "      Col64     Col65     Col66     Col67     Col68     Col69     Col70  \\\n",
       "0  0.075351  0.083303  0.105279  0.122436  0.147619  0.173012  0.202367   \n",
       "1  0.387991  0.352324  0.329852  0.296575  0.269225  0.244826  0.221323   \n",
       "2  0.086026  0.094625  0.116709  0.134746  0.157458  0.184183  0.211012   \n",
       "3  0.679261  0.696407  0.728218  0.741674  0.754156  0.773425  0.783040   \n",
       "4  0.416926  0.408196  0.419126  0.415662  0.416564  0.422136  0.426737   \n",
       "\n",
       "      Col71     Col72     Col73     Col74     Col75     Col76     Col77  \\\n",
       "0  0.228276  0.261486  0.291620  0.332648  0.370159  0.394883  0.437931   \n",
       "1  0.193631  0.174319  0.153871  0.138538  0.126047  0.111024  0.107718   \n",
       "2  0.236778  0.269691  0.297031  0.338024  0.374058  0.396542  0.424092   \n",
       "3  0.778321  0.760194  0.736431  0.738694  0.717196  0.679276  0.667349   \n",
       "4  0.424942  0.424252  0.420807  0.432660  0.434141  0.423763  0.430360   \n",
       "\n",
       "      Col78     Col79     Col80     Col81     Col82     Col83     Col84  \\\n",
       "0  0.472219  0.503045  0.644568  0.679630  0.719550  0.761869  0.791958   \n",
       "1  0.098914  0.100119  0.117977  0.114455  0.126901  0.145431  0.162011   \n",
       "2  0.419071  0.418398  0.502887  0.498856  0.502334  0.510534  0.509377   \n",
       "3  0.649871  0.624775  0.729130  0.699493  0.670210  0.640456  0.594668   \n",
       "4  0.429589  0.426454  0.511867  0.506561  0.508878  0.515521  0.512974   \n",
       "\n",
       "      Col85     Col86     Col87     Col88     Col89     Col90     Col91  \\\n",
       "0  0.819152  0.852555  0.879120  0.893838  0.907680  0.925014  0.928071   \n",
       "1  0.174786  0.200064  0.224692  0.253898  0.276524  0.313691  0.346178   \n",
       "2  0.506423  0.512530  0.513782  0.513323  0.508779  0.513844  0.511625   \n",
       "3  0.543652  0.498765  0.449014  0.401807  0.353460  0.322486  0.291414   \n",
       "4  0.508473  0.512937  0.512677  0.510651  0.504567  0.508472  0.504779   \n",
       "\n",
       "      Col92     Col93     Col94     Col95     Col96     Col97     Col98  \\\n",
       "0  0.940588  0.938938  0.929698  0.925659  0.911138  0.901213  0.875550   \n",
       "1  0.384944  0.415450  0.446948  0.492227  0.531260  0.568777  0.603168   \n",
       "2  0.516758  0.511680  0.505718  0.512800  0.513804  0.512731  0.510337   \n",
       "3  0.269180  0.239442  0.212153  0.198469  0.179317  0.147562  0.123794   \n",
       "4  0.508609  0.502063  0.494843  0.500868  0.501093  0.499040  0.496161   \n",
       "\n",
       "      Col99    Col100    Col101    Col102    Col103    Col104    Col105  \\\n",
       "0  0.855242  0.825250  0.794845  0.761984  0.728347  0.688468  0.646426   \n",
       "1  0.642756  0.674139  0.705839  0.737792  0.768454  0.790191  0.812307   \n",
       "2  0.513910  0.510933  0.507928  0.509681  0.507167  0.504482  0.501387   \n",
       "3  0.106391  0.086919  0.070197  0.071778  0.068560  0.084542  0.100753   \n",
       "4  0.499173  0.495863  0.492577  0.494096  0.491377  0.489089  0.486187   \n",
       "\n",
       "     Col106    Col107    Col108    Col109    Col110    Col111    Col112  \\\n",
       "0  0.609315  0.566886  0.527835  0.478071  0.439570  0.394203  0.358181   \n",
       "1  0.836668  0.851576  0.866683  0.870724  0.882243  0.878812  0.882164   \n",
       "2  0.504091  0.502154  0.503805  0.495279  0.499354  0.493262  0.497225   \n",
       "3  0.128092  0.154373  0.183158  0.198231  0.225472  0.242533  0.270998   \n",
       "4  0.489559  0.488141  0.490440  0.482910  0.487890  0.482879  0.488122   \n",
       "\n",
       "     Col113    Col114    Col115    Col116    Col117    Col118    Col119  \\\n",
       "0  0.322800  0.282667  0.239611  0.212284  0.184046  0.149749  0.124382   \n",
       "1  0.885688  0.873178  0.854639  0.838139  0.825352  0.800076  0.775432   \n",
       "2  0.501470  0.497814  0.489541  0.490670  0.493688  0.487575  0.486500   \n",
       "3  0.300569  0.329826  0.359554  0.405393  0.456404  0.501284  0.551075   \n",
       "4  0.493356  0.491004  0.484357  0.486947  0.491515  0.487117  0.487343   \n",
       "\n",
       "     Col120    Col121    Col122    Col123    Col124    Col125    Col126  \\\n",
       "0  0.109429  0.095443  0.078435  0.074675  0.062121  0.063165  0.263747   \n",
       "1  0.746250  0.723682  0.686454  0.653969  0.615141  0.584738  0.553114   \n",
       "2  0.486791  0.491376  0.486201  0.488421  0.483289  0.488394  0.494236   \n",
       "3  0.598253  0.646652  0.677641  0.708715  0.730951  0.760745  0.788017   \n",
       "4  0.489267  0.495464  0.491593  0.495359  0.491531  0.498033  0.505202   \n",
       "\n",
       "     Col127    Col128  \n",
       "0  0.114574  0.400647  \n",
       "1  0.507915  0.468949  \n",
       "2  0.487169  0.486169  \n",
       "3  0.801661  0.820882  \n",
       "4  0.500369  0.516898  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Data/training_data.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad175985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   output\n",
       "0       5\n",
       "1       1\n",
       "2       4\n",
       "3       3\n",
       "4       4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.read_csv('Data/y_train.csv')\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177a39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/testing_data.csv')\n",
    "y_test = pd.read_csv('Data/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a42fee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\harsh\\anaconda4\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: clang~=5.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: keras~=2.6 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\harsh\\anaconda4\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e333233",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.replace({1:0,\n",
    "                2:1,\n",
    "                3:2,\n",
    "                4:3,\n",
    "                5:4}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82259d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "301f0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, input_dim = 128, activation = 'relu'),\n",
    "    keras.layers.Dense(64, activation = 'relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(32, activation = 'relu'),\n",
    "    keras.layers.Dense(16, activation = 'relu'),\n",
    "    keras.layers.Dense(5, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a8b04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ccb1678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "252/252 [==============================] - 0s 716us/step - loss: 1.4727 - accuracy: 0.3018\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 0s 712us/step - loss: 1.1600 - accuracy: 0.4188\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 0s 719us/step - loss: 1.1422 - accuracy: 0.4211\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 0s 735us/step - loss: 1.1407 - accuracy: 0.4242\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 0s 722us/step - loss: 1.1379 - accuracy: 0.4283\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 0s 727us/step - loss: 1.1400 - accuracy: 0.4225\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 0s 717us/step - loss: 1.1372 - accuracy: 0.4272\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 0s 716us/step - loss: 1.1364 - accuracy: 0.4305\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 0s 743us/step - loss: 1.1431 - accuracy: 0.4272\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 0s 736us/step - loss: 1.1347 - accuracy: 0.4268\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 0s 707us/step - loss: 1.1336 - accuracy: 0.4275\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 0s 706us/step - loss: 1.1336 - accuracy: 0.4316\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 0s 703us/step - loss: 1.1335 - accuracy: 0.4261\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 0s 716us/step - loss: 1.1341 - accuracy: 0.4307\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 0s 703us/step - loss: 1.1334 - accuracy: 0.4334\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 0s 719us/step - loss: 1.1354 - accuracy: 0.4295\n",
      "Epoch 17/100\n",
      "252/252 [==============================] - 0s 714us/step - loss: 1.1334 - accuracy: 0.4295\n",
      "Epoch 18/100\n",
      "252/252 [==============================] - 0s 727us/step - loss: 1.1324 - accuracy: 0.4292\n",
      "Epoch 19/100\n",
      "252/252 [==============================] - 0s 710us/step - loss: 1.1311 - accuracy: 0.4390\n",
      "Epoch 20/100\n",
      "252/252 [==============================] - 0s 653us/step - loss: 1.1309 - accuracy: 0.4370\n",
      "Epoch 21/100\n",
      "252/252 [==============================] - 0s 629us/step - loss: 1.1306 - accuracy: 0.4329\n",
      "Epoch 22/100\n",
      "252/252 [==============================] - 0s 689us/step - loss: 1.1302 - accuracy: 0.4372\n",
      "Epoch 23/100\n",
      "252/252 [==============================] - 0s 656us/step - loss: 1.1284 - accuracy: 0.4380\n",
      "Epoch 24/100\n",
      "252/252 [==============================] - 0s 647us/step - loss: 1.1215 - accuracy: 0.4470\n",
      "Epoch 25/100\n",
      "252/252 [==============================] - 0s 650us/step - loss: 1.1153 - accuracy: 0.4538\n",
      "Epoch 26/100\n",
      "252/252 [==============================] - 0s 671us/step - loss: 1.0877 - accuracy: 0.4664\n",
      "Epoch 27/100\n",
      "252/252 [==============================] - 0s 678us/step - loss: 1.0328 - accuracy: 0.4964\n",
      "Epoch 28/100\n",
      "252/252 [==============================] - 0s 683us/step - loss: 0.9469 - accuracy: 0.5470\n",
      "Epoch 29/100\n",
      "252/252 [==============================] - 0s 649us/step - loss: 0.8809 - accuracy: 0.5799\n",
      "Epoch 30/100\n",
      "252/252 [==============================] - 0s 660us/step - loss: 0.8364 - accuracy: 0.6079\n",
      "Epoch 31/100\n",
      "252/252 [==============================] - 0s 640us/step - loss: 0.7729 - accuracy: 0.6376\n",
      "Epoch 32/100\n",
      "252/252 [==============================] - 0s 648us/step - loss: 0.7823 - accuracy: 0.6326\n",
      "Epoch 33/100\n",
      "252/252 [==============================] - 0s 664us/step - loss: 0.7309 - accuracy: 0.6556\n",
      "Epoch 34/100\n",
      "252/252 [==============================] - 0s 648us/step - loss: 0.7079 - accuracy: 0.6686\n",
      "Epoch 35/100\n",
      "252/252 [==============================] - 0s 660us/step - loss: 0.7320 - accuracy: 0.6579\n",
      "Epoch 36/100\n",
      "252/252 [==============================] - 0s 643us/step - loss: 0.6939 - accuracy: 0.6777\n",
      "Epoch 37/100\n",
      "252/252 [==============================] - 0s 633us/step - loss: 0.6669 - accuracy: 0.6851\n",
      "Epoch 38/100\n",
      "252/252 [==============================] - 0s 642us/step - loss: 0.6306 - accuracy: 0.7027\n",
      "Epoch 39/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.6159 - accuracy: 0.7107\n",
      "Epoch 40/100\n",
      "252/252 [==============================] - 0s 655us/step - loss: 0.6114 - accuracy: 0.7115\n",
      "Epoch 41/100\n",
      "252/252 [==============================] - 0s 650us/step - loss: 0.6033 - accuracy: 0.7129\n",
      "Epoch 42/100\n",
      "252/252 [==============================] - 0s 636us/step - loss: 0.6363 - accuracy: 0.7030\n",
      "Epoch 43/100\n",
      "252/252 [==============================] - 0s 634us/step - loss: 0.6120 - accuracy: 0.7107\n",
      "Epoch 44/100\n",
      "252/252 [==============================] - 0s 648us/step - loss: 0.5874 - accuracy: 0.7237\n",
      "Epoch 45/100\n",
      "252/252 [==============================] - 0s 647us/step - loss: 0.5718 - accuracy: 0.7295\n",
      "Epoch 46/100\n",
      "252/252 [==============================] - 0s 640us/step - loss: 0.6228 - accuracy: 0.7096\n",
      "Epoch 47/100\n",
      "252/252 [==============================] - 0s 643us/step - loss: 0.5659 - accuracy: 0.7314\n",
      "Epoch 48/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.5435 - accuracy: 0.7419\n",
      "Epoch 49/100\n",
      "252/252 [==============================] - 0s 640us/step - loss: 0.5514 - accuracy: 0.7396\n",
      "Epoch 50/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.5174 - accuracy: 0.7509\n",
      "Epoch 51/100\n",
      "252/252 [==============================] - 0s 646us/step - loss: 0.5381 - accuracy: 0.7452\n",
      "Epoch 52/100\n",
      "252/252 [==============================] - 0s 647us/step - loss: 0.4888 - accuracy: 0.7618\n",
      "Epoch 53/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.5064 - accuracy: 0.7539\n",
      "Epoch 54/100\n",
      "252/252 [==============================] - 0s 634us/step - loss: 0.4694 - accuracy: 0.7697\n",
      "Epoch 55/100\n",
      "252/252 [==============================] - 0s 640us/step - loss: 0.5007 - accuracy: 0.7588\n",
      "Epoch 56/100\n",
      "252/252 [==============================] - 0s 633us/step - loss: 0.4704 - accuracy: 0.7696\n",
      "Epoch 57/100\n",
      "252/252 [==============================] - 0s 651us/step - loss: 0.4745 - accuracy: 0.7652\n",
      "Epoch 58/100\n",
      "252/252 [==============================] - 0s 671us/step - loss: 0.4478 - accuracy: 0.7787\n",
      "Epoch 59/100\n",
      "252/252 [==============================] - 0s 645us/step - loss: 0.4665 - accuracy: 0.7692\n",
      "Epoch 60/100\n",
      "252/252 [==============================] - 0s 643us/step - loss: 0.4436 - accuracy: 0.7793\n",
      "Epoch 61/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.4367 - accuracy: 0.7813\n",
      "Epoch 62/100\n",
      "252/252 [==============================] - 0s 641us/step - loss: 0.4247 - accuracy: 0.7891\n",
      "Epoch 63/100\n",
      "252/252 [==============================] - 0s 636us/step - loss: 0.4731 - accuracy: 0.7696\n",
      "Epoch 64/100\n",
      "252/252 [==============================] - 0s 658us/step - loss: 0.4491 - accuracy: 0.7797\n",
      "Epoch 65/100\n",
      "252/252 [==============================] - 0s 648us/step - loss: 0.4393 - accuracy: 0.7825\n",
      "Epoch 66/100\n",
      "252/252 [==============================] - 0s 641us/step - loss: 0.4007 - accuracy: 0.7961\n",
      "Epoch 67/100\n",
      "252/252 [==============================] - 0s 640us/step - loss: 0.4459 - accuracy: 0.7782\n",
      "Epoch 68/100\n",
      "252/252 [==============================] - 0s 648us/step - loss: 0.4027 - accuracy: 0.7944\n",
      "Epoch 69/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.3914 - accuracy: 0.7982\n",
      "Epoch 70/100\n",
      "252/252 [==============================] - 0s 644us/step - loss: 0.3982 - accuracy: 0.7990\n",
      "Epoch 71/100\n",
      "252/252 [==============================] - 0s 642us/step - loss: 0.4012 - accuracy: 0.7990\n",
      "Epoch 72/100\n",
      "252/252 [==============================] - 0s 643us/step - loss: 0.4018 - accuracy: 0.7963\n",
      "Epoch 73/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.4197 - accuracy: 0.7910\n",
      "Epoch 74/100\n",
      "252/252 [==============================] - 0s 640us/step - loss: 0.3756 - accuracy: 0.8019\n",
      "Epoch 75/100\n",
      "252/252 [==============================] - 0s 642us/step - loss: 0.3630 - accuracy: 0.8128\n",
      "Epoch 76/100\n",
      "252/252 [==============================] - 0s 656us/step - loss: 0.4318 - accuracy: 0.7869\n",
      "Epoch 77/100\n",
      "252/252 [==============================] - 0s 646us/step - loss: 0.4029 - accuracy: 0.7945\n",
      "Epoch 78/100\n",
      "252/252 [==============================] - 0s 644us/step - loss: 0.4581 - accuracy: 0.7800\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 0s 656us/step - loss: 0.3406 - accuracy: 0.8276\n",
      "Epoch 80/100\n",
      "252/252 [==============================] - 0s 588us/step - loss: 0.3612 - accuracy: 0.8159\n",
      "Epoch 81/100\n",
      "252/252 [==============================] - 0s 640us/step - loss: 0.3347 - accuracy: 0.8308\n",
      "Epoch 82/100\n",
      "252/252 [==============================] - 0s 671us/step - loss: 0.3944 - accuracy: 0.7996\n",
      "Epoch 83/100\n",
      "252/252 [==============================] - 0s 645us/step - loss: 0.3330 - accuracy: 0.8384\n",
      "Epoch 84/100\n",
      "252/252 [==============================] - 0s 637us/step - loss: 0.3104 - accuracy: 0.8477\n",
      "Epoch 85/100\n",
      "252/252 [==============================] - 0s 656us/step - loss: 0.3354 - accuracy: 0.8561\n",
      "Epoch 86/100\n",
      "252/252 [==============================] - 0s 649us/step - loss: 0.2815 - accuracy: 0.8756\n",
      "Epoch 87/100\n",
      "252/252 [==============================] - 0s 636us/step - loss: 0.3231 - accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.2502 - accuracy: 0.8879\n",
      "Epoch 89/100\n",
      "252/252 [==============================] - 0s 641us/step - loss: 0.2564 - accuracy: 0.8990\n",
      "Epoch 90/100\n",
      "252/252 [==============================] - 0s 640us/step - loss: 0.3318 - accuracy: 0.8623\n",
      "Epoch 91/100\n",
      "252/252 [==============================] - 0s 648us/step - loss: 0.2402 - accuracy: 0.9005\n",
      "Epoch 92/100\n",
      "252/252 [==============================] - 0s 644us/step - loss: 0.3247 - accuracy: 0.8785\n",
      "Epoch 93/100\n",
      "252/252 [==============================] - 0s 642us/step - loss: 0.3338 - accuracy: 0.8597\n",
      "Epoch 94/100\n",
      "252/252 [==============================] - 0s 652us/step - loss: 0.2405 - accuracy: 0.9015\n",
      "Epoch 95/100\n",
      "252/252 [==============================] - 0s 647us/step - loss: 0.2190 - accuracy: 0.9123\n",
      "Epoch 96/100\n",
      "252/252 [==============================] - 0s 641us/step - loss: 0.2213 - accuracy: 0.9122\n",
      "Epoch 97/100\n",
      "252/252 [==============================] - 0s 656us/step - loss: 0.2285 - accuracy: 0.9061\n",
      "Epoch 98/100\n",
      "252/252 [==============================] - 0s 639us/step - loss: 0.2065 - accuracy: 0.9173\n",
      "Epoch 99/100\n",
      "252/252 [==============================] - 0s 642us/step - loss: 0.2111 - accuracy: 0.9133\n",
      "Epoch 100/100\n",
      "252/252 [==============================] - 0s 648us/step - loss: 0.2130 - accuracy: 0.9132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f3a22d8cd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e986d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.replace({1:0,\n",
    "                2:1,\n",
    "                3:2,\n",
    "                4:3,\n",
    "                5:4}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7207950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 432us/step - loss: 0.1193 - accuracy: 0.9639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11927837878465652, 0.9638888835906982]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9749ea9",
   "metadata": {},
   "source": [
    "**So our ANN model is giving an accuracy of 0.96 on the testing data**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
